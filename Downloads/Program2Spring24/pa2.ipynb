{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, data_dir, anno_csv) -> object:\n",
    "        self.anno_data = pd.read_csv(anno_csv)\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anno_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name = self.anno_data.iloc[idx, 0]\n",
    "        data_location = self.data_dir + data_name\n",
    "        data = np.float32(np.load(data_location))\n",
    "        # This is for one-hot encoding of the output label\n",
    "        gt_y = np.float32(np.zeros(10))\n",
    "        index = self.anno_data.iloc[idx, 1]\n",
    "        gt_y[index] = 1\n",
    "        return data, gt_y\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        indices = np.arange(0, len(self))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        batch_indices = np.array_split(indices, batch_size)\n",
    "        for i in range(0, len(self.anno_data), batch_size):\n",
    "            batch_indices = range(i, min(i + batch_size, len(self.anno_data)))\n",
    "            batch_data = [self.__getitem__(idx) for idx in batch_indices]\n",
    "            batch_data, batch_gt_y = zip(*batch_data)\n",
    "            yield np.array(batch_data), np.array(batch_gt_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.empty(784, 100)\n",
    "        init.xavier_uniform_(self.w1)\n",
    "        self.b1 = torch.rand(100,1)\n",
    "        self.w2 = torch.empty(100,100)\n",
    "        init.xavier_uniform_(self.w2)\n",
    "        self.b2 = torch.rand(100,1) \n",
    "        self.w3 = torch.empty(100,10)\n",
    "        init.xavier_uniform_(self.w3)\n",
    "        self.b3 = torch.rand(10,1)\n",
    "        self.eye_10 = torch.eye(10).unsqueeze(2)\n",
    "        self.eye_100 = torch.eye(100).unsqueeze(2)\n",
    "\n",
    "    def ReLU(self,Z):\n",
    "        return torch.clamp(Z, min=0)\n",
    "    def d_reLU(self,Z):\n",
    "        result = torch.zeros(Z.shape[0],Z.shape[0])\n",
    "        for i in range(Z.shape[0]):\n",
    "            for j in range(Z.shape[0]):\n",
    "                if i == j and Z[i] > 0:\n",
    "                    result[i][j] = 1\n",
    "        return result\n",
    "\n",
    "    def forward_propogation(self,X,Y):\n",
    "        X = X.unsqueeze(1)\n",
    "        Z1 = torch.matmul(self.w1.T,X) + self.b1\n",
    "        H1 = self.ReLU(Z1)\n",
    "        Z2 = torch.matmul(self.w2.T,H1) + self.b2\n",
    "        H2 = self.ReLU(Z2)\n",
    "        Z3 = torch.matmul(self.w3.T,H2) + self.b3\n",
    "        output = self.stable_softmax(Z3)\n",
    "        loss = self.loss(Y,output)\n",
    "        return Z1,H1,Z2,H2,Z3,output,loss\n",
    "\n",
    "    def loss(self, Y,Y_pred):\n",
    "        total_loss = 0\n",
    "        for k in range(10):\n",
    "            log_pred = torch.log(Y_pred[k])\n",
    "            total_loss += Y[k] * log_pred\n",
    "        return -1* total_loss\n",
    "\n",
    "        \n",
    "        \n",
    "    def gradient_loss(self,Y,Y_pred):\n",
    "        delta_y = torch.zeros(10,1)\n",
    "        for k in range(10):\n",
    "            delta_y[k] = -1 * Y[k]/Y_pred[k]\n",
    "\n",
    "        return delta_y\n",
    "    def stable_softmax(self,x):\n",
    "        return torch.exp(x - torch.max(x)) / torch.sum(torch.exp(x - torch.max(x)), axis=0)\n",
    "        \n",
    "    def deriv_softmax(self,y_pred):\n",
    "        y_pred = y_pred.squeeze()\n",
    "        diag_y_pred = torch.diag(y_pred)\n",
    "        outer_y_pred = torch.ger(y_pred, y_pred)\n",
    "        d_softmax = diag_y_pred - outer_y_pred\n",
    "    \n",
    "        return d_softmax\n",
    "    def backward_propogation(self,X,Z1,H1,Z2,H2,Z3,Y_pred,Y):\n",
    "        delta_y = self.gradient_loss(Y, Y_pred)\n",
    "        dZ3_w3 = (self.eye_10 * H2.squeeze(1))\n",
    "        dZ3_h2 = self.w3\n",
    "        d_softmax = self.deriv_softmax(Y_pred)\n",
    "        delta_w3_0 = torch.matmul(d_softmax,delta_y)\n",
    "\n",
    "        delta_w3 = torch.einsum(\"kkn, kj-> nk\",dZ3_w3, delta_w3_0)\n",
    "      \n",
    "        delta_h2 = torch.matmul(dZ3_h2, delta_w3_0)\n",
    "        \n",
    "        dZ2_w2 = (self.eye_100 * H1.squeeze(1))\n",
    "        dZ2_h1 = self.w2\n",
    "        d_reLU2 = self.d_reLU(Z2)\n",
    "\n",
    "        delta_w2_0 = torch.matmul(d_reLU2, delta_h2)\n",
    "        delta_w2 = torch.einsum(\"kkn, kj -> nk\",dZ2_w2, delta_w2_0)\n",
    "        delta_h1 = torch.matmul(dZ2_h1, delta_w2_0)\n",
    "\n",
    "        dZ2_w1 = (self.eye_100 * X).transpose(1,2)\n",
    "        \n",
    "        d_reLU1 = self.d_reLU(Z1)\n",
    "        delta_w1_0 = torch.matmul(d_reLU1, delta_h1)\n",
    "        delta_w1 = torch.einsum(\"nkn, nj -> nk\",dZ2_w1,delta_w1_0)\n",
    "\n",
    "        return delta_w3, delta_w3_0, delta_w2, delta_w2_0, delta_w1, delta_w1_0\n",
    "    \n",
    "    def update(self, lr, delta_w3, delta_w3_0, delta_w2, delta_w2_0, delta_w1, delta_w1_0):\n",
    "        self.w3 -= lr* delta_w3\n",
    "        self.b3 -= lr*delta_w3_0 \n",
    "        self.w2 -= lr*delta_w2.squeeze()\n",
    "        self.b2 -= lr*delta_w2_0\n",
    "        self.w1 -= lr* delta_w1.squeeze().T\n",
    "        self.b1 -= lr* delta_w1_0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def PA2_train():\n",
    "    # Specifying the training directory and label files\n",
    "    train_dir = './'\n",
    "    train_anno_file = './data_prog2Spring24/labels/train_anno.csv'\n",
    "\n",
    "    # Specifying the device to GPU/CPU. Here, GPU means 'cuda' and CPU means 'cpu'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Read the data and labels from the training data\n",
    "    MNIST_training_dataset = My_dataset(data_dir=train_dir, anno_csv=train_anno_file)\n",
    "    #You can set up your own maximum epoch. You may need  5 or 10 epochs to have a correct model.\n",
    "    my_max_epoch = 10\n",
    "    epochs = np.arange(0, my_max_epoch)\n",
    "    nn = NN()\n",
    "    acc_acc_list = []\n",
    "    test_acc_acc_list = []\n",
    "    avg_loss_list = []\n",
    "    for epoch in range(2):\n",
    "        train_avg_loss = 0\n",
    "        batches = MNIST_training_dataset.get_batch(50)\n",
    "        \n",
    "        accuracy_for_shown_up = [0] * 10\n",
    "        num_shown_up = [0] * 10\n",
    "        accuracy = 0\n",
    "        total_batch = 1000\n",
    "        b = 0\n",
    "        total_loss = 0\n",
    "        for batch in batches:\n",
    "            delta_w3 = 0\n",
    "            delta_w3_0 = 0\n",
    "            delta_w2 = 0 \n",
    "            delta_w2_0= 0\n",
    "            delta_w1 = 0 \n",
    "            delta_w1_0 = 0\n",
    "            b+=1\n",
    "            print(f\"Batch --> {b}\")\n",
    "            for m in range(len(batch[0])):\n",
    "                X = torch.from_numpy(batch[0][m])\n",
    "                Y = torch.from_numpy(batch[1][m])\n",
    "\n",
    "                all_out = nn.forward_propogation(X,Y)\n",
    "                # with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "                #     with record_function(\"model_inference\"):\n",
    "                num_shown_up[int(torch.argmax(Y))]+=1\n",
    "                if torch.argmax(all_out[5]) == torch.argmax(Y):\n",
    "                    accuracy_for_shown_up[int(torch.argmax(Y))]+=1\n",
    "                    accuracy+=1\n",
    "                total_loss += all_out[6]\n",
    "                bp = nn.backward_propogation(X,all_out[0],all_out[1],all_out[2],all_out[3],all_out[4],all_out[5],Y)\n",
    "                delta_w3 += bp[0]\n",
    "                delta_w3_0+= bp[1]\n",
    "                delta_w2+= bp[2]\n",
    "                delta_w2_0+= bp[3]\n",
    "                delta_w1 += bp[4]\n",
    "                delta_w1_0 += bp[5]\n",
    "            nn.update(.1, delta_w3/50, delta_w3_0/50, delta_w2/50, delta_w2_0/50, delta_w1/50, delta_w1_0/50)\n",
    "        \n",
    "        \n",
    "        acc_list = [0] * 10\n",
    "        for i in range(len(num_shown_up)):\n",
    "            acc_list[i] = (accuracy_for_shown_up[i]/num_shown_up[i])\n",
    "        acc_acc_list.append(acc_list)\n",
    "        avg_loss = total_loss/MNIST_training_dataset.__len__()\n",
    "        avg_loss_list.append(avg_loss)\n",
    "\n",
    "        filehandler = open(\"multiclass_parameters.txt\", \"wb\")\n",
    "        pickle.dump(nn,filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        test_acc = PA2_test()\n",
    "        test_acc_acc_list.append(test_acc)\n",
    "\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(len(test_acc_acc_list[0])):\n",
    "        plt.title(f'Test Epoch vs {i+1} Accuracy')\n",
    "\n",
    "        plt.plot([test_acc_acc_list[0][i],test_acc_acc_list[1][i]])\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(len(acc_acc_list[0])):\n",
    "        plt.title(f'Train Epoch vs {i+1} Accuracy')\n",
    "\n",
    "        plt.plot([acc_acc_list[0][i],acc_acc_list[1][i]])\n",
    "        plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Train Loss over Epoch\")\n",
    "    plt.plot(avg_loss_list)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        #Take the mean of all the mini-batch losses and denote it as your loss of the current epoch\n",
    "        #Collect loss for each epoch and save the parameter Theta after each epoch\n",
    "\n",
    "\n",
    "    # Plot the training loss vs accuracy\n",
    "    # Visualize the final weight matrix\n",
    "    # Save the final weight matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PA2_test():\n",
    "    # Specifying the training directory and label files\n",
    "    test_dir = './'\n",
    "    test_anno_file = './data_prog2Spring24/labels/test_anno.csv'\n",
    "    feature_length = 784\n",
    "    # Specifying the device to GPU/CPU. Here, GPU means 'cuda' and CPU means 'cpu'\n",
    "    filehandler = open(\"multiclass_parameters.txt\", \"rb\")\n",
    "    nn = pickle.load(filehandler)\n",
    "    #Load the Weight Matrix that has been saved after training\n",
    "\n",
    "\n",
    "    # Read the data and labels from the testing data\n",
    "    MNIST_testing_dataset = My_dataset(data_dir=test_dir, anno_csv=test_anno_file)\n",
    "    accuracy_for_shown_up = [0] * 10\n",
    "    num_shown_up = [0] * 10\n",
    "    accuracy = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    batches = MNIST_testing_dataset.get_batch(50)\n",
    "    total_batch = 1000\n",
    "    b = 0\n",
    "    accuracy = 0\n",
    "    for batch in batches:\n",
    "        for m in range(len(batch[0])):\n",
    "            X = torch.from_numpy(batch[0][m])\n",
    "            Y = torch.from_numpy(batch[1][m])\n",
    "            all_out = nn.forward_propogation(X,Y)\n",
    "            # with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "            #     with record_function(\"model_inference\"):\n",
    "            y_pred_list.append(int(torch.argmax(all_out[5]))+1)\n",
    "            y_list.append( int(torch.argmax(Y))+1)\n",
    "            num_shown_up[int(torch.argmax(Y))]+=1\n",
    "            if torch.argmax(all_out[5]) == torch.argmax(Y):\n",
    "                accuracy_for_shown_up[int(torch.argmax(Y))]+=1\n",
    "                accuracy+=1\n",
    "    acc_list = [0] * 10\n",
    "    for i in range(len(num_shown_up)):\n",
    "        acc_list[i] = (accuracy_for_shown_up[i]/num_shown_up[i])\n",
    "    print(acc_list)\n",
    "    print(y_list)\n",
    "    print(y_pred_list)\n",
    "    matrix_confusion = confusion_matrix(y_list, y_pred_list)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = matrix_confusion)\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    return accuracy/ MNIST_testing_dataset.__len__(), acc_list\n",
    "\n",
    "\n",
    "    # Predict Y using X and updated W.\n",
    "\n",
    "    # Calculate accuracy,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "PA2_train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
